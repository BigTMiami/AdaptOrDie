# -*- coding: utf-8 -*-
"""unipelt_classification_task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hgyn7DSYTJc24pOvylBatuLjuZgYeoQT
"""

from google.colab import drive
drive.mount("/content/drive")

!pip install datasets
!pip install transformers[torch]
!pip install adapters
!pip install scikit-learn

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)

    # Calculate accuracy
    accuracy = accuracy_score(labels, preds)

   # Calculate precision, recall, and F1-score
    f1 = f1_score(labels, preds, average='macro')

    return {
        'accuracy': accuracy,
        'f1_macro': f1
    }

from datasets import load_dataset
dataset_name = "BigTMiami/amazon_helpfulness"
dataset = load_dataset(dataset_name)

#print(train_dataset)
#print(eval_dataset)

#print_gpu_utilization()

from transformers import AutoTokenizer, DataCollatorWithPadding

tokenizer = AutoTokenizer.from_pretrained("roberta-base")
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

"""# Unipelt"""

from huggingface_hub import notebook_login

notebook_login()

from transformers import RobertaConfig
from adapters import AutoAdapterModel

config = RobertaConfig.from_pretrained("roberta-base")
model = AutoAdapterModel.from_pretrained(
    "roberta-base",
    config=config,
)
#print_gpu_utilization()

adapter_hub_name = "unipelt_adapter_classification_trained"

# Load without head, we are adding one in the
adapter_name = model.load_adapter("jgrc3/RobertaDAPT_adapters_unipelt", with_head=False)

# Add head for masked language modeling
model.add_classification_head(
    adapter_name,
    num_labels=2,
    id2label={ 0: "unhelpful", 1: "helpful"}, overwrite_ok=True
  )

# Set the adapter to be used for training
model.train_adapter(adapter_name)

train_dataset = dataset["train"]
eval_dataset = dataset["dev"]
#print_gpu_utilization()

from transformers import TrainingArguments
from adapters import AdapterTrainer

training_args = TrainingArguments(
    output_dir="./adapter_unipelt_classifier_training_output",
    learning_rate=1e-4,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_steps=100,
    overwrite_output_dir=True,
    # The next line is important to ensure the dataset labels are properly passed to the model
    remove_unused_columns=False,
)

trainer = AdapterTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["dev"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

from transformers import TrainingArguments, EarlyStoppingCallback
from adapters import AdapterTrainer

training_args = TrainingArguments(
    output_dir="./adapter_unipelt_classifier_training_output",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5, # Paper: this is for Classification, not domain training
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    # Lyudmila: changed from 3 to 10 -> used for small roberta
    # Lyudmila: changed back to 3 as agreed
    num_train_epochs=10,
    weight_decay=0.01,
    warmup_ratio=0.06, # Paper: warmup proportion of 0.06
    adam_epsilon=1e-6, # Paper 1e-6 (huggingface default 1e-08)
    adam_beta1=0.9, # Paper: Adam weights 0.9
    adam_beta2=0.98, # Paper: Adam weights 0.98 (huggingface default  0.999)
    lr_scheduler_type="linear",
    save_total_limit=2, # Saves latest 2 checkpoints
    push_to_hub=True,
    hub_strategy="checkpoint", # Only pushes at end with save_model()
    # Lyudmila: Changed to true -> ot seems according to repo and paper that they used early stopping and used best model
    # Lyudmila: Changed to false as agreed
    load_best_model_at_end=True, #Set to false - we want the last trained model like the paper
    # torch_compile=torch_compile,  # Much Faster
    logging_strategy="steps", # Is default
    logging_steps=100, # Logs training progress
    metric_for_best_model='f1_macro'
)

# EarlyStoppingCallback with patience
early_stopping = EarlyStoppingCallback(early_stopping_patience=3) # from paper
# callbacks=[early_stopping],

trainer = AdapterTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["dev"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

# if evaluate_on_test:
#     print("Evaluating model on test set")
#     test_evaluation_result = trainer.evaluate(dataset["test"])
#     print("Test set eval: {0}".format(test_evaluation_result))

eval_results = trainer.evaluate(dataset["test"])
# trained_eval_loss = eval_results["eval_loss"]
# print(f"Trained Eval Loss: {trained_eval_loss:.2f}")
# eval_f1_macro = eval_results["eval_f1_macro"]
# print(f"Trained eval_f1_macro:{100.0 * eval_f1_macro:.2f} Eval Loss: {eval_loss:.2f}")
print(eval_results)

training_results = trainer.train()

eval_results = trainer.evaluate(dataset["test"])
# eval_loss = eval_results["eval_loss"]
# eval_f1_macro = eval_results["eval_f1_macro"]
# print(f"Trained eval_f1_macro:{100.0 * eval_f1_macro:.2f} Eval Loss: {eval_loss:.2f}")
print(eval_results)

model.push_adapter_to_hub(
    adapter_hub_name,
    adapter_name,
    datasets_tag=dataset_name
)

"""# Pfeiffer"""

from huggingface_hub import notebook_login

notebook_login()

from transformers import RobertaConfig
from adapters import AutoAdapterModel

config = RobertaConfig.from_pretrained("roberta-base")
model = AutoAdapterModel.from_pretrained(
    "roberta-base",
    config=config,
)
#print_gpu_utilization()

from transformers import AutoTokenizer, DataCollatorWithPadding

tokenizer = AutoTokenizer.from_pretrained("roberta-base")
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

adapter_hub_name = "pfeiffer_adapter_classification_trained_10epochs"

# Load without head, we are adding one in the
adapter_name = model.load_adapter("jgrc3/RobertaDAPT_adapters_pfeiffer_reDo", with_head=False)

# Add head for masked language modeling
model.add_classification_head(
    adapter_name,
    num_labels=2,
    id2label={ 0: "unhelpful", 1: "helpful"}, overwrite_ok=True
  )

# Set the adapter to be used for training
model.train_adapter(adapter_name)

summary = model.adapter_summary()
print(summary)

summary

from transformers import TrainingArguments, EarlyStoppingCallback
from adapters import AdapterTrainer

training_args = TrainingArguments(
    output_dir="./adapter_pfeifferv2_classifier_training_output",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5, # Paper: this is for Classification, not domain training
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    # Lyudmila: changed from 3 to 10 -> used for small roberta
    # Lyudmila: changed back to 3 as agreed
    num_train_epochs=10,
    weight_decay=0.01,
    warmup_ratio=0.06, # Paper: warmup proportion of 0.06
    adam_epsilon=1e-6, # Paper 1e-6 (huggingface default 1e-08)
    adam_beta1=0.9, # Paper: Adam weights 0.9
    adam_beta2=0.98, # Paper: Adam weights 0.98 (huggingface default  0.999)
    lr_scheduler_type="linear",
    save_total_limit=2, # Saves latest 2 checkpoints
    push_to_hub=True,
    hub_strategy="checkpoint", # Only pushes at end with save_model()
    # Lyudmila: Changed to true -> ot seems according to repo and paper that they used early stopping and used best model
    # Lyudmila: Changed to false as agreed
    load_best_model_at_end=True, #Set to false - we want the last trained model like the paper
    # torch_compile=torch_compile,  # Much Faster
    logging_strategy="steps", # Is default
    logging_steps=100, # Logs training progress
    metric_for_best_model='f1_macro'
)

# EarlyStoppingCallback with patience
early_stopping = EarlyStoppingCallback(early_stopping_patience=3) # from paper
# callbacks=[early_stopping],

trainer = AdapterTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["dev"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

# if evaluate_on_test:
#     print("Evaluating model on test set")
#     test_evaluation_result = trainer.evaluate(dataset["test"])
#     print("Test set eval: {0}".format(test_evaluation_result))

eval_results = trainer.evaluate(dataset["test"])
eval_results

training_results = trainer.train()

eval_results = trainer.evaluate(dataset["test"])
eval_results

model.push_adapter_to_hub(
    adapter_hub_name,
    adapter_name,
    datasets_tag=dataset_name
)

"""# Unipelt No Pre"""

from huggingface_hub import notebook_login

notebook_login()

from transformers import RobertaConfig
from adapters import AutoAdapterModel

config = RobertaConfig.from_pretrained("roberta-base")
model = AutoAdapterModel.from_pretrained(
    "roberta-base",
    config=config,
)
#print_gpu_utilization()

adapter_hub_name = "unipelt_adapter_classification_noPre"
adapter_name = "classifier_unipelt_no_pretraining"
adapter_type = "unipelt" # could be "lora", etc.

# Add a new adapter
model.add_adapter(adapter_name, config=adapter_type)

# Add head for classification modeling
model.add_classification_head(
    adapter_name,
    num_labels=2,
    id2label={ 0: "unhelpful", 1: "helpful"})

# Set the adapter to be used for training
model.train_adapter(adapter_name)

summary = model.adapter_summary()
summary



from transformers import TrainingArguments, EarlyStoppingCallback
from adapters import AdapterTrainer

training_args = TrainingArguments(
    output_dir="./adapter_unipeltNone_classifier_training_output",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5, # Paper: this is for Classification, not domain training
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    # Lyudmila: changed from 3 to 10 -> used for small roberta
    # Lyudmila: changed back to 3 as agreed
    num_train_epochs=10,
    weight_decay=0.01,
    warmup_ratio=0.06, # Paper: warmup proportion of 0.06
    adam_epsilon=1e-6, # Paper 1e-6 (huggingface default 1e-08)
    adam_beta1=0.9, # Paper: Adam weights 0.9
    adam_beta2=0.98, # Paper: Adam weights 0.98 (huggingface default  0.999)
    lr_scheduler_type="linear",
    save_total_limit=2, # Saves latest 2 checkpoints
    push_to_hub=True,
    hub_strategy="checkpoint", # Only pushes at end with save_model()
    # Lyudmila: Changed to true -> ot seems according to repo and paper that they used early stopping and used best model
    # Lyudmila: Changed to false as agreed
    load_best_model_at_end=True, #Set to false - we want the last trained model like the paper
    # torch_compile=torch_compile,  # Much Faster
    logging_strategy="steps", # Is default
    logging_steps=100, # Logs training progress
    metric_for_best_model='f1_macro'
)

# EarlyStoppingCallback with patience
early_stopping = EarlyStoppingCallback(early_stopping_patience=3) # from paper
# callbacks=[early_stopping],

uni_no_trainer = AdapterTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["dev"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

eval_results = uni_no_trainer.evaluate(dataset["test"])
eval_results

training_results = uni_no_trainer.train()

eval_results = uni_no_trainer.evaluate(dataset["test"])
eval_results

model.push_adapter_to_hub(
    adapter_hub_name,
    adapter_name,
    datasets_tag=dataset_name
)

"""# Pfeiffer No Pre Redo"""

from huggingface_hub import notebook_login

notebook_login()

from transformers import RobertaConfig
from adapters import AutoAdapterModel

config = RobertaConfig.from_pretrained("roberta-base")
model = AutoAdapterModel.from_pretrained(
    "roberta-base",
    config=config,
)
#print_gpu_utilization()

adapter_hub_name = "pfeiffer_adapter_classification_noPre_10epochs_redo"
adapter_name = "classifier_pfeiffer_no_pretraining"
adapter_type = "pfeiffer" # could be "lora", etc.

# Add a new adapter
model.add_adapter(adapter_name, config=adapter_type)

# Add head for classification modeling
model.add_classification_head(
    adapter_name,
    num_labels=2,
    id2label={ 0: "unhelpful", 1: "helpful"})

# Set the adapter to be used for training
model.train_adapter(adapter_name)

summary = model.adapter_summary()
summary

from transformers import TrainingArguments, EarlyStoppingCallback
from adapters import AdapterTrainer

training_args = TrainingArguments(
    output_dir="./adapter_pfeifferNoPre_redo_classifier_training_output",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5, # Paper: this is for Classification, not domain training
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    # Lyudmila: changed from 3 to 10 -> used for small roberta
    # Lyudmila: changed back to 3 as agreed
    num_train_epochs=10,
    weight_decay=0.01,
    warmup_ratio=0.06, # Paper: warmup proportion of 0.06
    adam_epsilon=1e-6, # Paper 1e-6 (huggingface default 1e-08)
    adam_beta1=0.9, # Paper: Adam weights 0.9
    adam_beta2=0.98, # Paper: Adam weights 0.98 (huggingface default  0.999)
    lr_scheduler_type="linear",
    save_total_limit=2, # Saves latest 2 checkpoints
    push_to_hub=True,
    hub_strategy="checkpoint", # Only pushes at end with save_model()
    # Lyudmila: Changed to true -> ot seems according to repo and paper that they used early stopping and used best model
    # Lyudmila: Changed to false as agreed
    load_best_model_at_end=True, #Set to false - we want the last trained model like the paper
    # torch_compile=torch_compile,  # Much Faster
    logging_strategy="steps", # Is default
    logging_steps=100, # Logs training progress
    metric_for_best_model='f1_macro'
)

# EarlyStoppingCallback with patience
early_stopping = EarlyStoppingCallback(early_stopping_patience=3) # from paper
# callbacks=[early_stopping],

pfeiffer_no_trainer = AdapterTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["dev"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

eval_results = pfeiffer_no_trainer.evaluate(dataset["test"])
eval_results

training_results = pfeiffer_no_trainer.train()

eval_results = pfeiffer_no_trainer.evaluate(dataset["test"])
eval_results

model.push_adapter_to_hub(
    adapter_hub_name,
    adapter_name,
    datasets_tag=dataset_name
)

"""# Updated Hyperparams

# Unipelt LR = 0.0001
"""

from huggingface_hub import notebook_login

notebook_login()

from transformers import RobertaConfig
from adapters import AutoAdapterModel

config = RobertaConfig.from_pretrained("roberta-base")
model = AutoAdapterModel.from_pretrained(
    "roberta-base",
    config=config,
)

adapter_hub_name = "unipelt_adapter_classification_trained_lr0_0001"

# Load without head, we are adding one in the
adapter_name = model.load_adapter("jgrc3/RobertaDAPT_adapters_unipelt", with_head=False)

# Add head for masked language modeling
model.add_classification_head(
    adapter_name,
    num_labels=2,
    id2label={ 0: "unhelpful", 1: "helpful"}, overwrite_ok=True
  )

# Set the adapter to be used for training
model.train_adapter(adapter_name)

summary = model.adapter_summary()
summary

from transformers import TrainingArguments, EarlyStoppingCallback
from adapters import AdapterTrainer

training_args = TrainingArguments(
    output_dir="./adapter_unipelt_classifier_training_output",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-4, # Paper: this is for Classification, not domain training
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    # Lyudmila: changed from 3 to 10 -> used for small roberta
    # Lyudmila: changed back to 3 as agreed
    num_train_epochs=10,
    weight_decay=0.01,
    warmup_ratio=0.06, # Paper: warmup proportion of 0.06
    adam_epsilon=1e-6, # Paper 1e-6 (huggingface default 1e-08)
    adam_beta1=0.9, # Paper: Adam weights 0.9
    adam_beta2=0.98, # Paper: Adam weights 0.98 (huggingface default  0.999)
    lr_scheduler_type="linear",
    save_total_limit=2, # Saves latest 2 checkpoints
    push_to_hub=True,
    hub_strategy="checkpoint", # Only pushes at end with save_model()
    # Lyudmila: Changed to true -> ot seems according to repo and paper that they used early stopping and used best model
    # Lyudmila: Changed to false as agreed
    load_best_model_at_end=True, #Set to false - we want the last trained model like the paper
    # torch_compile=torch_compile,  # Much Faster
    logging_strategy="steps", # Is default
    logging_steps=100, # Logs training progress
    metric_for_best_model='f1_macro'
)

# EarlyStoppingCallback with patience
early_stopping = EarlyStoppingCallback(early_stopping_patience=3) # from paper
# callbacks=[early_stopping],

uni_pre_trainer = AdapterTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["dev"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping]
)

eval_results = uni_pre_trainer.evaluate(dataset["test"])
eval_results

training_results = uni_pre_trainer.train()

eval_results = uni_pre_trainer.evaluate(dataset["test"])
eval_results

model.push_adapter_to_hub(
    adapter_hub_name,
    adapter_name,
    datasets_tag=dataset_name
)

