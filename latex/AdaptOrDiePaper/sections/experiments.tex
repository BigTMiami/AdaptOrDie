As in Gururangan et al., we evaluate model classification performance using the F1-Macro score. This is a good measure for imbalanced data as the ``Macro" setting weights each class equally.

\subsection{Reproducing Experiments}
The goal of the first set of experiments was to replicate the result reported by Gururangan et al.

\subsubsection{Reproducing: Pretraining}
\textbf{Experiments Setup.}
The first step was to reproduce the pretraining stage. We tested multiple configurations of pretraining: DAPT on 25M reviews, DAPT on 5M reviews, TAPT on full data with 20 epochs (1), and TAPT on 20\% data (2). For each configuration, we performed the MLM task with a masking probability of 15\% as is in the original paper. We pretrained the model for 1 epoch (full pass) for DAPT, 20 epochs for TAPT 1, and 100 epochs for TAPT 2. See the details in the \textbf{Appendix \ref{appendix:train_config}}. We tested multiple configurations of DAPT and TAPT to assess whether there is an improvement in a subset of training data and to further decide if we can proceed with a reduced amount of data for adapters to speed up the experiments.

\textbf{Results and Analysis}. 
The masked LM loss was calculated before and after pretraining; see \textbf{Appendix} \ref{appendix:masked_lm_adapters} for details. The \textbf{loss was decreased} on each configuration, which indicated the effectiveness of the pretraining. The impact on the downstream task was then assessed with further experiments.

\subsubsection{Reproducing: Classification}

\textbf{Experiments Setup.}
The next step was to assess the effect of the pretraining step on the downstream classification task. Our main focus was on the \textbf{Helpfulness} task, where using pretrained models showed a more significant impact. We trained the classifier for 10 epochs and chose the best model based on F1-Macro on the validation set; we then calculated the F1-Macro on the testing set; see the details in the \textbf{Appendix}.

\textbf{Results and Analysis.}
Table \ref{table:2} shows the results of the classification (averaged across 5 runs).

\begin{table*}[ht]
\centering
\caption{Full Fine-Tuning Results (Helpfulness Task): F1-Macro}
\small  % This reduces the font size
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
& \textbf{RoBERTa}& \textbf{DAPT (5M)} & \textbf{DAPT (25M)} & \textbf{TAPT} & \textbf{DAPT/TAPT} \\
\hline
\hline
Out Results & $69.09_{0.65}$ & $69.10_{1.37}$ & $\mathbf{69.74_{0.70}}$ &  TAPT 1: $69.23_{0.83}$, TAPT 2: $69.49_{1.01}$ & $68.37_{0.70}$ \\
Original Paper & $65.10_{3.4}$ & --- & $66.5_{1.4}$ & $69.2_{2.4}$ & $\mathbf{69.4_{2.1}}$ \\
\hline
\end{tabular}
\label{table:2}
\end{table*}

We observed that there is an \textbf{improvement in most of the experiments with pretrained models} compared to the baseline RoBERTa, which supports the results of the original paper. A \textbf{slight decrease} was observed in the DAPT + TAPT model. Pretraining on both domain and task data may result in more complex dynamics in language modeling. In the original paper, the DAPT+TAPT also did not always lead to a greater improvement compared to either DAPT or TAPT used alone. The difference with the paper may be explained by the difference in the DAPT dataset used for pretraining.

The \textbf{average F1-Macro of the baseline RoBERTa is higher} than the \textbf{one reported} in the original paper and is closer to the pretrained-based results: \textbf{69.09} compared to the \textbf{65.10 }reported in the paper. 
The updated optimizer could lead to some changes in the training results. Also, the authors of the original paper do not specify which configurations were used to compare downstream tasks and whether they used the same configuration for the RoBERTa base classifier and pretrained models. The authors mention that they used “3 or 10 epochs” of training. The original repository also includes different configurations: small (10 epochs, batch size 16), mini (10 epochs, batch size 8), and big (3 epochs, batch size 16). When training with a “big” option we got an average F1-Macro of $\mathbf{67.40_{1.47}}$, which is closer to what was reported in the paper. However, the lower metric is explained by the model to be \textbf{under-trained}. After completing our research and examining additional literature, we found a paper \cite{he2021}, where the authors independently got a similar difference in the baseline metric. They reported the F1-Macro of $\mathbf{69.1_{0.6}}$ for the baseline RoBERTa classification which is very close to what we got.

We decided to use the same configuration of the classification model that allows the model to converge (10 epochs, 16 batch size) for both RoBERTa and task- or domain-adapted models to mitigate the influence of the configuration, and focus on the impact of the pretraining stage itself; see the detailed configuration we used in \textbf{Appendix} \ref{appendix:train_config}.

To support the observed improvement in the baseline with pretraining, we also performed a subset of experiments on the second IMDB downstream task reported in the paper. Table \ref{table:3} shows the results. As for the Helpfulness, we got an improvement in the baseline with DAPT pretraining on both 5M and 25M reviews. The baseline and improved metrics are close to those reported in the original paper.

\begin{table}[ht]
\centering
\caption{Full Fine-Tuning Classification Results (IMDB Task): F1-Macro}
\small  % This reduces the font size
\begin{tabular}{|l|c|c|c|}
\hline
& \textbf{RoBERTa}& \textbf{DAPT (5M)} & \textbf{DAPT (25M)} \\
\hline
\hline
Out Results & 95.16 & 95.33 & \textbf{95.70} \\
Original Paper & 95.0 & 95.4 & \textbf{95.5} \\
\hline
\end{tabular}
\label{table:3}
\end{table}

\subsection{Adapters Experiments}
The goal of this set of experiments was to repeat the steps outlined in the original paper but to use adapters for the DAPT and TAPT tasks instead of performing an entire model pretraining and fine-tuning.

\subsubsection{Adapters: Pretraining}
\textbf{Experiments Setup.}
Similar to the reproducing part, we tested multiple configurations of pretraining with adapters: DAPT (5M), TAPT 1 (100\%, 20 epochs), TAPT 2 (20\%, 100 epochs), and DAPT + TAPT on 5M reviews and 20\% of task data.

For each configuration, we performed the MLM task with a masking probability of 15\%. We used the same number of pretraining epochs as for the full pertaining. While for the full pretraining, the whole set of weights has been updated, with adapters, only a small subset of weights was tuned. See the details in \textbf{Appendix} \ref{appendix:train_config}.

\textbf{Results and Analysis.}
As for the full pertaining, there is a decrease in masked LM loss, indicating effective pretraining using adapters; see results in \ref{appendix:masked_lm_adapters}. 

\subsubsection{Adapters: Downstream Classification}
\textbf{Experiments Setup.}
The next step was to assess the effect of the pretraining step on the downstream classification task using adapters. In contrast to full pretraining, the classification did not require loading a whole pretrained model. The trained adapter and classification head were added to the base Roberta model; the uploaded adapter was then trained, when the main model weights were kept frozen.

The same training configuration was used here as for a full fine-tuning model with the following difference: the learning rate of \textbf{0.0001} was used for adapters instead of the learning rate of \textbf{0.00002} used for the full fine-tuning; see the Adapter Tuning: Learning Rate for details.

\textbf{Results and Analysis}.
Table \ref{table:4} shows the results of the classification.

\begin{table}[ht]
\centering
\caption{Adapter-Based Fine-Tuning Results (Helpfulness Task): F1-Macro}
\small  % This reduces the font size
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{RoBERTa}& \textbf{DAPT} & \textbf{TAPT 1} & \textbf{TAPT 2} & \textbf{DAPT/TAPT} \\
\hline
\hline
 \multicolumn{5}{|c|}{\textbf{SeqBN}} \\
\hline
68.81 & 69.43 & \textbf{70.27} & 69.50 & 69.87  \\
\hline
 \multicolumn{5}{|c|}{\textbf{Unipelt}} \\
\hline
69.89 & 69.21 & 70.43 & \textbf{70.83 }& 68.68  \\
\hline
\end{tabular}
\label{table:4}
\end{table}

The baseline RoBERTa metrics \textbf{were reproduced} with both types of adapters. Also, the baseline RoBERTa results \textbf{were improved} for most of the configurations with pretraining. \textbf{Both SeqBN and Unipelt configurations} where \textbf{improved with TAPT pertaining}. \textbf{Unipelt} adapter shows a \textbf{slight worsening} with \textbf{DAPT}, and \textbf{DAPT/TAPT pretraining}.

The following reasons might explain the difference between SeqBN and Unipelt: the baseline metric is slightly higher for Unipelt, which may indicate that using a larger and more complex configuration can help with improving the baseline; considering more complex adapter and more weights to be trained, the DAPT for Unipelt may require more data to boost the metrics. For TAPT the dataset is smaller, however, the data is more specific to the downstream task, which can explain why even a small set gives some improvement here; DAPT+TAPT model may not always lead to the improvement even if DAPT or TAPT does for the similar reasons as for the full-pretraining.

\subsubsection{Adapters Tuning: Learning Rate}
\textbf{Experiments Setup.}
The goal of these experiments was to evaluate the effect of the hyper-parameter tuning of the adapters. What was observed is that tuning the learning rate has a significant effect on the classification performance.

\textbf{Results and Analysis.}
Table \ref{table:5} shows the results of the experiments. In contrast to a full fine-tuning, where all the weights are affected by training, with adapters, only a small subset of weights are fine-tuned. With full fine-tuning, a \textbf{higher learning rate} may lead to forgetting the pretraining knowledge, while there is no such risk for adapters. Also, for the base RoBERTa, the adapters are initialized with random weights, which may require a higher learning rate to converge to the optimal results.
\begin{table}[]
\centering
\caption{Adapters Classification Results on Different Learning Rates (Helpfulness Task): F1-Macro}
\small  % This reduces the font size
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{RoBERTa}& \textbf{DAPT (5M)} & \textbf{RoBERTa} & \textbf{DAPT (5M)}\\
\textbf{lr: 2.00E-05}& \textbf{lr: 2.00E-05} & \textbf{lr: 1.00E-04} & \textbf{lr: 1.00E-04}\\
\hline
\hline
 \multicolumn{4}{|c|}{\textbf{SeqBN}} \\
\hline
65.14 & 66.50& 68.81 & \textbf{69.89}  \\
\hline
 \multicolumn{4}{|c|}{\textbf{Unipelt}} \\
\hline
66.04 & 66.00 & \textbf{69.43} & 69.21  \\
\hline
\end{tabular}
\label{table:5}
\end{table}
\subsubsection{Adapters Configurations: One Adapter vs Two Adapters}
\textbf{Experiments Setup.}
An additional experiment was run to compare the following approaches: using the same adapter for both TAPT pretraining and further classification and adding an additional adapter to the classification stage while freezing the previously pretrained adapter.

\textbf{Results and Analysis.}
See Table \ref{table:6} for results. The results show that there is \textbf{no significant difference} between the two configurations. Adding an adapter for SeqBN led to a slight increase in classification metrics while adding an additional adapter to Unipelt led to a slight decrease. The difference may be explained by the size and the difference in architecture of the adapters. For example, Unipelt has more weight and is more complex, and it may be the case that using a new adapter initialized randomly for classification requires more data to get improvement. At the same time, the SeqBN adapter, which is smaller, may benefit from freezing the pretrained adapter and adding a classification adapter on top of the pretrained one.

\begin{table}[ht]
\centering
\caption{Adapters Classification Results on Different Configurations (Helpfulness Task): F1-Macro}
\small  % This reduces the font size
\begin{tabular}{|c|c|}
\hline
\textbf{TAPT: Same Adapter}& \textbf{TAPT: Add Adapter}\\
\hline
\hline
 \multicolumn{2}{|c|}{\textbf{SeqBN}} \\
\hline
70.27 & \textbf{70.38  }\\
\hline
 \multicolumn{2}{|c|}{\textbf{Unipelt}} \\
\hline
\textbf{70.43} & 70.15  \\
\hline
\end{tabular}
\label{table:6}
\end{table}

\subsubsection{Adapters Scaling: TAPT on Micro-Sets Experiments}
\textbf{Experiments Setup.}
The goal of this set of experiments was to evaluate the effect of the TAPT pretraining on classification performance by varying the amount of training.

\textbf{Results and Analysis.}
First, we investigated if we would see a TAPT benefit with the smaller Micro Help dataset using 1, 5, and 10 epochs of pretraining. When using 1 epoch, we reduced the batch size and removed gradient accumulation so that we would have more training steps. \textbf{Figure \ref{fig:micro_help_class_vs_pretraining_epochs} } shows that we were able to produce a \textbf{positive result} with \textbf{just 1 epoch}. 10 epochs showed a more significant improvement, but 5 showed slightly less distinct boost. We can see the standard deviation on 5 epochs is much higher, so we did another experiment looking at more pretraining.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/micro_help_class_vs_pretraining_epochs.png}
\caption{This shows the results of TAPT on the Amazon Helpfulness 5k reviews dataset using different amounts of pretraining.}
\label{fig:micro_help_class_vs_pretraining_epochs}
\end{figure}

\textbf{Figure \ref{fig:micro_class_vs_epochs} } shows that the evaluation loss of the masked language model pretraining does not seem to be correlated to classification results. As loss goes down, classification results move up and down.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/micro_class_vs_epochs.png}
\caption{This shows the results of classification with three different datasets before and after Task Adaptive PreTraining (TAPT)}
\label{fig:micro_class_vs_epochs}
\end{figure}

Having established that just 1 epoch of pretraining is effective, we wanted to confirm that worked for other datasets. \textbf{Figure \ref{fig:micro_class_1_epoch_pretrain} } shows that 1 epoch of pretraining improved classification results for all three datasets, which provides a diversity of size, data balance, and domain.  

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/micro_class_1_epoch_pretrain.png}
\caption{This shows the positive impact of doing only 1 epoch of TAPT pretraining across three different datasets, which are of different size, balanced and imbalanced}
\label{fig:micro_class_1_epoch_pretrain}
\end{figure}

The classification using no TAPT uses a randomly initialized adapter. Was the pretraining step simply creating a word focused initialization that really was not specific to the task data? In order to verify it, we ran another experiment, where we TAPT pre-trained model on relevant and irrelevant task data. We had a very useful finding, which supports that the TAPT pre-training is effective in this setting only if used with relevant task data. \textbf{Figure \ref{fig:micro_class_pretrain_datasets} } compares the Micro Help classification task using different pretrained adapters. This shows that classification only improved with task-specific pretraining, not with pre-training on different sets of task data.  

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/micro_help_class_cross_pretrain.png}
\caption{This shows classification results for the Amazon Helpfulness 5k reviews dataset using different TAPT pretrained models.}
\label{fig:micro_class_pretrain_datasets}
\end{figure}