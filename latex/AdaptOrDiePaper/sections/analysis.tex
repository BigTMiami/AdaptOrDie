In the "micro" experiments, we demonstrated that even small amounts of pre training with task classification data improves classification when using an adapter architecture. A future area for review would be determining if Masked Language Modeling concurrent with classification can improve classification.  It is not clear how to develop an efficient parallel architecture to do this because the tasks uses somewhat different data structures.

