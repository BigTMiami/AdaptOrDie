% Large Language Models (LLM's) have become the popular face of deep learning and AI through ChatGPT, Llama, and others. Even after training for months on billions of tokens, performance can be improved with fine-tuning using relevant data, as shown in Gururangan et al. While promising, full pre-training and fine-tuning are costly and may be accompanied by problems, such as catastrophic forgetting.

Transformer-based large language models form the basis for much of the AI-revolution seen today, but often require extra tuning using relevant data to perform competitively in specific tasks. Existing methods, such as full-pretraining and fine-tuning work well but are costly and accompanied by other problems. In this paper, we explore a lightweight adapter-based alternative for fine-tuning LLMs to specific domains and tasks. We replicate the full-model tuning baselines and demonstrate the positive effects of domain and task-adaptive adapter training in comparison. We also explore different adapter architectures and evaluate the lowest amount of tuning that produces a performance gain. We show promising results that performance can be boosted even with small amounts of task-relevant data.