Large Language Models (LLM's) have become the popular face of deep learning and AI through ChatGPT, Llama and others.  Even after training for months on billions of tokens, performance can be improved with fine-tuning as shown in Gururangan et. al. 2020. Adapters are a lightweight architecture for fine-tuning LLM's.  This paper looks at replicating  Gururangan et. al. 2020's fine-tuning performance gains using adapter architectures.  We look at various adapters as well as exploring the lowest amount of fine tuning that produces a performance gain.