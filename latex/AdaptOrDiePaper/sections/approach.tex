Our project had three phases: reproducing the results of the Gururangan et al., replicating the results with adapters, and exploring potential novel benefits of using adapters outside the scope of Gururangan et al. research.

\subsection{Reproducing Training Pipeline}

Our first goal was to reproduce results from Gururangan et al., which would allow us to create a nuts and bolts understanding of the training pipeline, ensure we could make it work, and set a baseline for further comparison with adapter-based methods.

\subsubsection{Pretraining}
\textbf{Dataset.} \textbf{For DAPT}, we used the \textbf{Amazon Review} data because we were able to identify what we think is a similar source. It was also the smallest of the domain datasets, which made experiments more manageable considering the limited computation resources.

Our first unexpected challenge was related to identifying the data source. Gururangan et al. reference the repository with the code base and data. However, while the Task data was available, the Domain data was not. Gururangan et al. also reference He et al. \cite{amazonData} as the Domain data source, which led to Ni et al. \cite{amazonData_Part_2} paper and a website with Amazon reviews. In Gururangan et al., the authors used \textbf{24.76M} reviews for domain adaptation. The He et al. site contained several versions; however, none matched this number. We chose a subset (\textbf{``5-core"}) of the \textbf{2018 dataset}, which we thought is closest to the original, comprising of \textbf{37 categories} with a total of \textbf{75 M reviews}. We sampled each category proportionally to get to \textbf{25M}.

A challenge we had anticipated was the computational load of running LLM models. The roberta-base model we used contains\textbf{~124M} parameters. To adhere to Google Colab memory constraints, we were forced to use smaller batch sizes. While this results in longer runtimes, we were able to optimize the process by grouping multiple reviews into a single 512-token roberta-base block. This follows as pretraining uses MLM which does not rely on labels. This resulted in including almost \textbf{6 reviews} on average in each row, \textbf{increasing training efficiency} by almost a \textbf{factor of 6} while still exposing the model to the same number of reviews without losing any information. One other improvement was the use of PyTorch compilation. Since version 2.0, PyTorch allows for the compilation of the computation graph, which, in our case, almost \textbf{doubled performance}, even with the associated overhead.

\textbf{For TAPT}, the task-specific data was used for the MLM fine-tuning. The dataset consists of the same samples that are further used for downstream classification (without labels). Just as we did for domain pretraining, we created a set of condensed datasets from the Task data to speed pretraining.

\textbf{Codebase.} Another unexpected challenge was the effective age of the Gururangan et al. codebase \cite{dontstoppretraining2020}. We instead implemented pretraining using the up-to-date Hugginface's Transformers API \cite{huggingface_transformers}, which allowed us to use standardized methods for deep learning tasks, such as tokenization, data collation, and training loops. We also used the Hugginface's API for storing and exchanging datasets and models \cite{datasets}. This modification also allowed for the sharing of models and datasets, providing great value for group research.  Our code, pretrained models, and datasets are publicly available \cite{github_repo}. 
% , as we could upload the pre-processed datasets and pre-trained models so that each team member could have access.

\subsubsection{Classification}
The impact of the DAPT and TAPT pretraining was then assessed on the downstream classification task.

\textbf{Dataset.}
For classification, we used three datasets: Amazon Helpfulness, IMDB Review Sentiment, and Citation Intent. \textbf{Figure \ref{fig:dataset_analysis}} shows two key aspects of these datasets. The primary dataset used was Amazon Helpfulness. This is both the largest as well as very imbalanced. We can also see that the Citation Intent dataset is quite imbalanced (as seen in figure \ref{fig:dataset_analysis}. 

% Gururangan et al. use an F1 Macro score for evaluation, which is a good measure for imbalanced data. The ``Macro" setting weights each class equally.

\textbf{Codebase.}
As with the pretraining stage, we faced challenges with the original repository being outdated. The classification code also relied on the AllenNLP framework which is no longer supported \cite{dontstoppretraining2020}. We were still able to run experiments using the original repo with two modifications: we replaced the unsupported BertAdam optimizer with the AdamW and used the last release of AllenNLP. We then replicated the same pipeline using Hugging Face's Transformers API \cite{huggingface_transformers} for the same task. Starting with an old code and switching to a new framework helped us to ensure we didn't introduce any new elements in the classification task.

We were able to get results close to those reported in the paper; see the details in the Experiments and Results sections.

\subsection {Adapters}
Having been able to reproduce the basic findings for DAPT and TAPT, we felt comfortable that we had good data and effective models and training. The next goal was to implement this using adapters. We chose to work with two adapter architectures at this point: SeqBN and UniPelt. The main goal was to compare the impact of smaller and larger and more complex adapters on the performance. For the DAPT task, we limited experiments to using 5M records of domain data, which helped us to significantly speed up the experiments.  

\textbf{Dataset and Codebase.} The same data sources were used while experimenting with adapters with some variations of the data size depending on the experiment setup. We used Huggingfaces' Transformers API to experiment with adapters.

We were able to replicate Gururangan et al. using the adapter architectures, as shown in the Experiment and Results section. Computational cost and time were a significant issue, shaping the final phase of our work.

\textbf{Exploration.}
While we were able to replicate positive pretraining effects using adapters, we were limited in our ability to explore different configurations using the original amount of data. We decided to focus on a reduced-size dataset of 5K ("Micro Help" dataset) reviews and TAPT; see \textbf{Figure \ref{fig:dataset_analysis} }. We liked that task data would be available to all users for classification tasks without having to consider finding the right domain dataset. We also wanted to explore the smallest amount of pretraining that would produce a positive impact. This joined our interest in a more tractable experimentation framework with the general users' interest in improving results with the least amount of effort. Finally, we thought we would use an ``intermediate" adapter architecture, parallel bottleneck, with the bottlenecks in parallel instead of sequentially, as proposed in He et al. \cite{he2022unified}. We used the same reduction factor as with the sequential bottleneck adapter, so it contained the same amount of parameters as seen in \textbf{Table \ref{tab:adapter_parameters}}.








