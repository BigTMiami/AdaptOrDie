The experiments showed that using adapter-based pretraining and fine-tuning can replicate the results reached with full pretraining and fine-tuning. This implies significant benefits. Adapters are relatively small and easily shareable; using adapters may mitigate some problems related to full fine-tuning, such as catastrophic forgetting; training adapters may take less time.

When using adapters, it is critical to perform hyper-parameter tuning. Our experiments demonstrated that using a higher learning rate compared to one used in full fine-tuning led to a significant increase in performance results. This makes sense considering the comparatively small size of adapters.

In our experiments, we focused on two architectures: SeqBN and Unipelt. Both SeqBN (more simple) and Unipelt (more complex) adapter architectures demonstrated similar results with somewhat higher baseline performance for the Unipelt as could be expected given the considerably larger complexity of Unipelt and training length.

In the "micro" experiments, we demonstrated that even small amounts of pretraining with task data improve classification when using an adapter architecture. A future area for review would be determining if Masked Language Modeling concurrent with classification can improve classification.  It is not clear how to develop an efficient parallel architecture to do this because the tasks use somewhat different data structures.

Another possible area of research is comparing full fine-tuning and adapter-based tuning for multi-task applications. The benefit of using adapters may be even more prominent here, as access to multiple-task data is not always available, and sequential training may lead to catastrophic forgetting. Adding and combining multi-purpose adapters may be highly beneficial in such scenarios. In the future, we would also like to compare the recall capabilities of fully pre-trained models to adapter-pre-trained models on non-pretrained domains.