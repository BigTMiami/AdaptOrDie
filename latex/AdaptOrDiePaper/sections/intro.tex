% make sure checks all 4 bullet points in intro rubric
Large language models (LLMs) powered by the transformer architecture form the foundation for a significant amount of the AI advancements today, enabling technologies such as AI chatbots, LLM-driven search engines, and even the discussion of artificial general intelligence \cite{vaswani2017attention}. These models consist of hundreds of millions of parameters trained on huge, heterogeneous corpora of data \cite{liu2019roberta}. They perform impressively when tested across general domains. However, they often require an additional push to perform competitively on a specific area or task, such as medical diagnosis \cite{wang2018glue, gururangan2020don}. 

% Taking an existing LLM, such as RoBERTa, and tuning it to a specific task proves challenging. 

This topic was addressed by Gururangan et al. in their 2020 paper ``Don't Stop Pretraining" \cite{gururangan2020don}, which proved the benefits of domain and task-adaptive pretraining (DAPT/TAPT). Pretraining describes the method by which an existing model can be trained on additional data to improve its recollection before being fine-tuned for a specific task. Gurungaran et al. were able to show that continued pretraining of the model on domain data (DAPT) consistently improved performance on classification tasks from that target domain. They also showed that pretraining the model on the actual data used in the classification task (TAPT) improved performance on that classification task, with or without being combined with DAPT \cite{gururangan2020don}. However, these current methods require tuning over the whole model, which is computationally expensive and can result in catastrophic forgetting.

In this paper, we explore an ‘adapter’-based alternative to domain and task-adaptive pretraining that would allow for fast, data-efficient model tuning. Adapters are small sub-networks that can be plugged into the existing transformer blocks. Instead of updating all the parameters of the pretrained language model, the original model is frozen, and only the parameters of the introduced sub-architecture are updated in learning \cite{poth2023adapters, pfeiffer2020adapterhub}. Adapters are incredibly parameter-efficient, often under 1\% of the full model's parameters, and modular. They have been shown previously to provide competitive performance with full fine-tuning in transfer learning \cite{pfeiffer2020adapterhub}.

We ask the question if can we get similar, if not better, performance by attaching and pretraining an adapter. We explore and compare the performance of adapters after DAPT, TAPT, and combined DAPT+TAPT on downstream classification tasks to that of full-model pretraining replicated from the Gururangan paper \cite{gururangan2020don}. We also look at the performance of adapter pretraining on reduced datasets, asking the question if adapters can more efficiently transfer learn from a small corpus of domain or task-specific data than a full transformer-based model. We look at multiple adapter architectures over different pretraining methods and dataset sizes.

Competitive adapter-based domain and task-adaptive pretraining methods would allow for efficient and modular transfer learning of existing LLMs to specific tasks. In addition, it would allow the composition of multiple pretrained adapters for highly competitive, single-model multi-subject performance. 

% Difference between adapters
\textbf{Adapters.} In our work, we evaluate the performance of small bottleneck adapters, like SeqBN, as well as large prefix-and-bottleneck combined UniPELT adapters over different pretraining methods and dataset sizes \cite{pfeiffer2020mad, mao2021unipelt}. As the base model, we use the pretrained RoBERTa LLM \cite{liu2019roberta}, a transformer-based architecture trained on over 160 GB of unlabeled raw text.

Bottleneck adapters consist of two normalized feed-forward layers, where one layer downscales the output and the other upscales. The Pfeiffer adapter is inserted after the feed-forward block in each Transformer layer \cite{pfeiffer2020mad}. As the RoBERTa base model consists of 12 stacks of Transformer-encoder layers, the RoBERTa with added Pfeiffer configuration will have 12 bottleneck adapters consisting of 894,528 added parameters in total \cite{pfeiffer2020mad, pfeiffer2020adapterhub}, as seen in \textbf{Table \ref{tab:adapter_parameters}}. In contrast, UniPELT combines multiple adapter methods, including LORA, Prefix Tuning, and bottleneck adapters, across different areas of the Transformer layer and implements a gating mechanism that controls submodule activation \cite{mao2021unipelt}. For the RoBERTa model, it has 11,083,376 parameters to be trained \cite{pfeiffer2020adapterhub}.
\begin{table}[]
\begin{center}
\caption{The parameter count used by different adapters.  Both the sequential and parallel bottleneck use a reduction factor of 16.}
\small
\begin{tabular}{ | c | r | r | }
\hline
Model Adapter & Parameters (M)  & \% of Model \\
\hline\hline
roberta-base model & 124.6 & 100.0\% \\
\hline\hline
Sequential Bottleneck & .9 & .7\% \\
\hline
Parallel Bottleneck & .9 & .7\% \\
\hline
UniPelt & 11.1 & 8.9\% \\
\hline
\end{tabular}
\end{center}
\label{tab:adapter_parameters}
\end{table}

\textbf{Datasets.} We use the Amazon Reviews dataset and the Amazon review helpfulness, IMBD review sentiment, and citation intent datasets \cite{amazonData,amazonHelpfulness,imbdData,citationIntentData}. The training sizes and label distributions of the classification datasets are shown in \textbf{Figure \ref{fig:dataset_analysis}}. 

\begin{figure}[!htb]
\centering
\includegraphics[scale = 0.4]{figures/dataset_analysis.png}
\caption{This shows the three datasets used in classification. The left shows the different sizes of training sets, while the right shows the percentage of label types.}
\label{fig:dataset_analysis}
\end{figure}
