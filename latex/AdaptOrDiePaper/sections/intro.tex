% make sure checks all 4 bullet points in intro rubric
Large language models (LLMs) powered by the transformer architecture form the foundation for a significant amount of the AI advancements and hype seen today, enabling technologies such as AI chat-bots, LLM-driven search engines, and even the discussion of artificial general intelligence \cite{vaswani2017attention}. These models consist of hundreds of millions of parameters trained on huge, heterogeneous corpuses of data \cite{liu2019roberta}. They perform impressively when tested across general domains. However, they often require an additional push to perform competitively on a specific area or task, such as medical diagnosis \cite{wang2018glue, gururangan2020don}. Taking an existing LLM, such as RoBERTa, and tuning it to a specific task proves challenging. 

This topic was addressed by Gururangan et. al. in their 2020 paper "Don't Stop Pretraining", which proved the benefits of domain-adaptive and task-adaptive pretraining (DAPT/TAPT) on multiple domains and classification tasks. Pre-training describes the step by which an existing model can be trained on additional data, often domain or task-specific, to improve its recollection before being fine-tuned for a specific task. Gurungaran et. al. were able to show that continued pretraining of the model on a domain (DAPT) consistently improved performance on classification tasks from that target domain \cite{gururangan2020don}. They also showed that pretraining the model on a much smaller but directly task-related corpus greatly improved performance on that classification task, with or without being combined with DAPT \cite{gururangan2020don}. However, these current methods require fine-tuning over the whole model, large domain and task-specific datasets, and computationally expensive training times.

In this paper, we explore an ‘adapter’-based alternative to this domain and task-adaptive pretraining that would allow for fast, data-efficient model tuning. Adapters are small sub-architectures that allow for efficient and lightweight fine-tuning of pre-trained transformer-based LLMs for specific, downstream tasks. Instead of updating all the parameters of the pre-trained language model when training, they freeze the existing weights and introduce a small sub-network of new parameters to the model to be updated in learning \cite{poth2023adapters, pfeiffer2020adapterhub}. Adapters are incredibly parameter-efficient, often under 1\% of the full model's parameters, and modular. They have been shown previously to provide competitive performance with full fine-tuning in transfer learning \cite{pfeiffer2020adapterhub}.

We ask the question if instead of tuning the entire model via DAPT and TAPT for some sub-task, can we get similar if not better performance by attaching and pretraining an adapter. We explore and compare the performance of adapters after DAPT, TAPT, and combined DAPT+TAPT on downstream classification tasks to that of full-model pretraining replicated from the Gururangan paper \cite{gururangan2020don}. We also look at the performance of adapter pretraining on reduced datasets, asking the question if adapters can more efficiently transfer learn from a small corpus of domain or task-specific data than a full transformer-based model. We look at multiple adapter architectures in our study, evaluating the performance of small bottleneck style adapters, like SeqBN, as well as large prefix-and-bottleneck combined UniPELT adapters over different pretraining methods and dataset sizes \cite{pfeiffer2020mad, mao2021unipelt}. 

% Difference between adapters
Bottleneck adapters consists of two normal feed forward layer, where one of the layers downscales the output and the other upscales. The Pfeiffer adapter we implement is inserted after the feed forward block in each Transformer layer \cite{pfeiffer2020mad}. As the RoBERTa base model consists of 12 stacks of Transformer-encoder layers, the RoBERTa with added Pfeiffer configuration will have of 12 bottleneck adapters consisting of 894,528 added parameters in total \cite{pfeiffer2020mad, pfeiffer2020adapterhub}. In contrast, UniPELT combines multiple adapter methods, including LORA, Prefix Tuning, and bottleneck adapters, across different areas of the Transformer layer and implements a gating mechanism that controls submodule activation \cite{mao2021unipelt}. For the RoBERTa model, it has 11,083,376 parameters to be trained \cite{pfeiffer2020adapterhub}.

Competitive adapter-based domain and task-adaptive pretraining methods would allow for efficient and modular transfer learning of existing LLMs to specific tasks. In addition, it would allow the composition of multiple pre-trained adapters for highly competitive, single-model multi-subject performance. 

For our project, we use the pre-trained RoBERTa large language model \cite{liu2019roberta}. It is a transformer-based architecture trained using masked language modeling objective on over 160 GB of unlabeled raw text. We attach additional adapter architectures to the model using AdapterHub \cite{pfeiffer2020adapterhub}. We use the Amazon REVIEWS dataset and the Amazon review helpfulness dataset for DAPT and TAPT respectively. The Amazon REVIEWS dataset consists of 24.75 million full Amazon reviews across different product domains, compiled in text form for the purpose of language model training \cite{amazonData, amazonData_Part_2}. This dataset was used for domain-adaptive pre-training using masked-language modeling. The Amazon review helpfulness dataset consists of 200,000 Amazon reviews with attached helpfulness labels to be used for positive/negative language classification \cite{amazonHelpfulness}. Our code, pre-trained models, and datasets are publicly available \cite{Hugging Face and Github}.
