{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook will create the imdb dataset for classification.  We don't pad these here, the data collator will do it on the fly.  We also don't condense because they are labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir('datasets/classifier/imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.6M  100 25.6M    0     0  4326k      0  0:00:06  0:00:06 --:--:-- 4154k     0  0:00:05  0:00:01  0:00:04 4735k00:02  0:00:03 5130k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6651k  100 6651k    0     0  4710k      0  0:00:01  0:00:01 --:--:-- 4713k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 31.4M  100 31.4M    0     0  11.2M      0  0:00:02  0:00:02 --:--:-- 11.2M\n"
     ]
    }
   ],
   "source": [
    "!curl -Lo train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/imdb/train.jsonl --output-dir 'datasets/classifier/imdb'\n",
    "!curl -Lo dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/imdb/dev.jsonl --output-dir 'datasets/classifier/imdb'\n",
    "!curl -Lo test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/imdb/test.jsonl --output-dir 'datasets/classifier/imdb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afm/.pyenv/versions/3.10.6/envs/adapt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 20000 examples [00:00, 480007.32 examples/s]\n",
      "Generating test split: 25000 examples [00:00, 664189.57 examples/s]\n",
      "Generating dev split: 5000 examples [00:00, 738850.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"datasets/classifier/imdb/train.jsonl\",\n",
    "        \"test\": \"datasets/classifier/imdb/test.jsonl\",\n",
    "        \"dev\": \"datasets/classifier/imdb/dev.jsonl\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_2136',\n",
       " 'text': \"I thought this film was just about perfect. The descriptions/summaries you'll read about this movie don't do it justice. The plot just does not sound very interesting, BUT IT IS. Just rent it and you will not be sorry!!\",\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn't used here because the labels are already integers, but can be used in training\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "    )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20000/20000 [00:03<00:00, 6107.53 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:03<00:00, 6525.01 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 6392.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This removes the text and id columns from the dataset as they are not needed\n",
    "dataset_tokens = dataset.map(preprocess_function, batched=True, remove_columns=[\"id\", \"text\"])\n",
    "dataset_tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100, 802, 42, 822, 21, 95, 59, 1969, 4, 20, 24173, 73, 29, 16598]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[\"train\"][4][\"input_ids\"][:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I thought this film was just about perfect. The descriptions/summaries you'll read about this movie don't do it justice. The plot just does not sound very interesting, BUT IT IS. Just rent it and you will not be sorry!!</s>\n",
      "I thought this film was just about perfect. The descriptions/summaries you'll read about this movie don't do it justice. The plot just does not sound very interesting, BUT IT IS. Just rent it and you will not be sorry!!\n"
     ]
    }
   ],
   "source": [
    "decoded_string =tokenizer.decode(dataset_tokens[\"train\"][4][\"input_ids\"])\n",
    "original_string = dataset[\"train\"][4][\"text\"]\n",
    "print(decoded_string)\n",
    "print(original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 117.67ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 25/25 [00:00<00:00, 123.81ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 95.74ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BigTMiami/imdb_sentiment_dataset/commit/aefb7cb57b743624ce7e22d9fa56843f22387405', commit_message='Upload dataset', commit_description='', oid='aefb7cb57b743624ce7e22d9fa56843f22387405', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens.push_to_hub(\"imdb_sentiment_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
