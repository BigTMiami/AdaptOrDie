{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook will create the imdb dataset for classification.  We don't pad these here, the data collator will do it on the fly.  We also don't condense because they are labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir('datasets/classifier/citation_intent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -Lo train.jsonl https://s3-us-west-2.amazonaws.com/allennlp/dont_stop_pretraining/data/citation_intent/train.jsonl --output-dir 'datasets/classifier/citation_intent'\n",
    "!curl -Lo dev.jsonl https://s3-us-west-2.amazonaws.com/allennlp/dont_stop_pretraining/data/citation_intent/dev.jsonl --output-dir 'datasets/classifier/citation_intent'\n",
    "!curl -Lo test.jsonl https://s3-us-west-2.amazonaws.com/allennlp/dont_stop_pretraining/data/citation_intent/test.jsonl --output-dir 'datasets/classifier/citation_intent'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afm/.pyenv/versions/3.11.3/envs/adapt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"datasets/classifier/citation_intent/train.jsonl\",\n",
    "        \"test\": \"datasets/classifier/citation_intent/test.jsonl\",\n",
    "        \"dev\": \"datasets/classifier/citation_intent/dev.jsonl\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'metadata'],\n",
       "        num_rows: 1688\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'metadata'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['text', 'label', 'metadata'],\n",
       "        num_rows: 114\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .', 'label': 'Background', 'metadata': {}}\n",
      "{'text': 'Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'label': 'CompareOrContrast', 'metadata': {}}\n",
      "{'text': 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'label': 'Background', 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "print(dataset[\"test\"][0])\n",
    "print(dataset[\"dev\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Background', 'Uses', 'CompareOrContrast', 'Extends', 'Motivation', 'Future']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset[\"train\"].to_pandas()\n",
    "labels = df['label'].unique().tolist()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Background': 0, 'Uses': 1, 'CompareOrContrast': 2, 'Extends': 3, 'Motivation': 4, 'Future': 5}\n",
      "{0: 'Background', 1: 'Uses', 2: 'CompareOrContrast', 3: 'Extends', 4: 'Motivation', 5: 'Future'}\n"
     ]
    }
   ],
   "source": [
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update labels\n",
    "dataset = dataset.map(lambda examples: {\"label\": label2id[examples[\"label\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .', 'label': 0, 'metadata': {}}\n",
      "{'text': 'Resnik ( 1995 ) reported a correlation of r = .9026.10 The results are not directly comparable , because he only used noun-noun pairs , words instead of concepts , a much smaller dataset , and measured semantic similarity instead of semantic relatedness .', 'label': 2, 'metadata': {}}\n",
      "{'text': 'Typical examples are Bulgarian ( Simov et al. , 2005 ; Simov and Osenova , 2003 ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .', 'label': 0, 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])\n",
    "print(dataset[\"test\"][0])\n",
    "print(dataset[\"dev\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/afm/.pyenv/versions/3.11.3/envs/adapt/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "    )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1688/1688 [00:00<00:00, 27862.66 examples/s]\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 19694.23 examples/s]\n",
      "Map: 100%|██████████| 114/114 [00:00<00:00, 18155.78 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1688\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 139\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 114\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This removes the text and id columns from the dataset as they are not needed\n",
    "dataset_tokens = dataset.map(preprocess_function, batched=True, remove_columns=[\"metadata\", \"text\"])\n",
    "dataset_tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'input_ids': [0, 42702, 2156, 81, 5, 375, 367, 107, 2156, 552, 19, 9766, 11, 5, 304, 9, 2239, 8, 17325, 6448, 13, 3857, 9, 455, 28564, 268, 36, 5415, 2156, 7528, 25606, 732, 4422, 20082, 2156, 7528, 102, 25606, 732, 4422, 20082, 2156, 7528, 428, 25606, 12041, 282, 1115, 3994, 3592, 2156, 7528, 4839, 2156, 1233, 2017, 34, 57, 156, 15, 5, 304, 9, 17325, 2239, 6448, 7, 5281, 16762, 46563, 8117, 45774, 28201, 22810, 50, 1617, 14, 4064, 11, 10, 45774, 28201, 1291, 36, 2197, 2156, 11151, 25606, 3513, 18086, 8, 7380, 2156, 7969, 25606, 19021, 22704, 4400, 1076, 4, 2156, 6708, 25606, 5866, 324, 8, 13891, 2156, 6708, 25606, 6760, 3979, 4400, 1076, 4, 2156, 6193, 25606, 14687, 219, 677, 260, 1638, 8, 13880, 2156, 5155, 25606, 19443, 9649, 329, 4400, 1076, 4, 2156, 6193, 25606, 255, 40435, 1636, 18002, 8, 19443, 9649, 329, 2156, 3788, 4839, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'label': 2, 'input_ids': [0, 20028, 8256, 36, 7969, 4839, 431, 10, 22792, 9, 910, 5457, 479, 3248, 2481, 4, 698, 20, 775, 32, 45, 2024, 10451, 2156, 142, 37, 129, 341, 44875, 12, 282, 7928, 15029, 2156, 1617, 1386, 9, 14198, 2156, 10, 203, 2735, 41616, 2156, 8, 9550, 46195, 37015, 1386, 9, 46195, 1330, 1825, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'label': 0, 'input_ids': [0, 47572, 3569, 7721, 32, 29586, 36, 6202, 1417, 4400, 1076, 4, 2156, 4013, 25606, 6202, 1417, 8, 384, 7305, 3900, 2156, 4999, 4839, 2156, 1111, 36, 10136, 4400, 1076, 4, 2156, 4999, 4839, 2156, 13501, 36, 229, 5638, 2279, 2156, 4999, 4839, 2156, 8, 9004, 36, 30060, 7815, 4400, 1076, 4, 2156, 4013, 4839, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_tokens[\"train\"][0])\n",
    "print(dataset_tokens[\"test\"][0])\n",
    "print(dataset_tokens[\"dev\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Briscoe and Carroll ( 1997 ) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX.</s>\n",
      "Briscoe and Carroll ( 1997 ) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX .\n"
     ]
    }
   ],
   "source": [
    "decoded_string =tokenizer.decode(dataset_tokens[\"train\"][4][\"input_ids\"])\n",
    "original_string = dataset[\"train\"][4][\"text\"]\n",
    "print(decoded_string)\n",
    "print(original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 506.80ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 566.87ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 520.58ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BigTMiami/ACL_ARC_dataset/commit/3483867088d05be089b41f3c2eb181b78636c87d', commit_message='Upload dataset', commit_description='', oid='3483867088d05be089b41f3c2eb181b78636c87d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens.push_to_hub(\"ACL_ARC_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
